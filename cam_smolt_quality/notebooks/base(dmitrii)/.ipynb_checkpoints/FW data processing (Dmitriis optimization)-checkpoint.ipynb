{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62cd008b-d730-4fbe-b404-50804dfd3d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.patches import Patch\n",
    "import os\n",
    "from shutil import copy\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import imageio\n",
    "from itertools import compress\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'legend.title_fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "#‘xx-small’, ‘x-small’, ‘small’, ‘medium’, ‘large’, ‘x-large’, ‘xx-large’\n",
    "pylab.rcParams.update(params)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.optimize import curve_fit\n",
    "from tslearn.clustering import KShape, silhouette_score\n",
    "\n",
    "import gc\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import math\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def weighted_avg(x, weight, factor):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        tmp = x[[weight, factor]].dropna()\n",
    "        weighted_sum = (tmp[weight] * tmp[factor]).sum()\n",
    "        count_sum = tmp[weight].sum()\n",
    "        return weighted_sum / count_sum\n",
    "\n",
    "#read data\n",
    "locus_weights=pd.read_csv('..\\\\data\\\\evt_movement_ratio_with_dates.csv')\n",
    "locus_weights.starttime = pd.to_datetime(locus_weights.starttime,format='%Y-%m-%d')\n",
    "locus_weights.endtime = pd.to_datetime(locus_weights.endtime,format='%Y-%m-%d')\n",
    "\n",
    "temperature = pd.read_csv('..\\\\data\\\\temperature_for_CAM.csv')\n",
    "temperature.event_date = pd.to_datetime(temperature.event_date,format='%Y-%m-%d')\n",
    "temperature.locus_group_id=temperature.locus_group_id.astype('int16')\n",
    "#not sure in row below as it 'converts' 12.3 -> 12.296875\n",
    "temperature.value=temperature.value.astype('float16')\n",
    "temperature['event_year']=temperature['event_date'].dt.year\n",
    "\n",
    "llg_match= pd.read_csv('..\\\\data\\\\locus_locus_group_matching.csv')\n",
    "llg_match.locus_id=llg_match.locus_id.astype('int32')\n",
    "llg_match.locus_group_id=llg_match.locus_group_id.astype('int16')\n",
    "\n",
    "df_dates=pd.read_csv('..\\\\data\\\\FW_cycle_dates.csv')\n",
    "for d in ['first_movement_date', 'first_feeding_date', 'shipout_date']:\n",
    "    df_dates[d] = pd.to_datetime(df_dates[d],format='%Y-%m-%d')\n",
    "    \n",
    "sfm = pd.read_csv('..\\\\data\\\\seawater_freshwater_matching.csv')\n",
    "sfm_ = sfm[sfm.origin_site_type=='Freshwater'][['target_seawater_locus_id','transport_date','ponding_date','pretransfer_fw_locus_population_id','fish_count_shipped_out','avg_weight_g_stocked']]\n",
    "sfm_.pretransfer_fw_locus_population_id=sfm_.pretransfer_fw_locus_population_id.astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d5e94-d0c9-445f-ae03-3c518fcfcb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lw_dates=locus_weights.groupby('final_locus_population_id').agg({'starttime':'min','endtime':'max'})\n",
    "lw_dates.starttime = pd.to_datetime(lw_dates.starttime,format='%Y-%m-%d')\n",
    "lw_dates.endtime = pd.to_datetime(lw_dates.endtime,format='%Y-%m-%d')\n",
    "#to be checked\n",
    "lw_dates['FW_cycle_length'] = (lw_dates.endtime - lw_dates.starttime).dt.days+1\n",
    "lw_dates['starttime_year']=lw_dates['starttime'].dt.year\n",
    "#we limit FW cycles to those started in 2017 because there are issues with temperature readings for 2015-2016\n",
    "lw_dates_2017=lw_dates[lw_dates.starttime_year>=2017]\n",
    "\n",
    "N = len(locus_weights['final_locus_population_id'].unique())\n",
    "rnd_idxs = np.random.randint(0, N, 50)\n",
    "mask = locus_weights['final_locus_population_id'].unique()[rnd_idxs]\n",
    "mask = locus_weights['final_locus_population_id'].isin(mask)\n",
    "\n",
    "#time consuming\n",
    "lw_alldates_list = []\n",
    "for ind, row in tqdm(locus_weights[mask].iterrows()):\n",
    "    for d in pd.date_range(row.starttime,row.endtime-datetime.timedelta(days=1)):\n",
    "        lw_alldates_list.append([row.final_locus_population_id,d,row.historic_locus_id,row.count_ratio])\n",
    "lw_alldates = pd.DataFrame(lw_alldates_list, columns = ['final_locus_population_id','event_date','historic_locus_id','weight0'])\n",
    "lw_alldates_weights_grouped=lw_alldates.groupby(['final_locus_population_id','event_date'])[['weight0']].sum().reset_index()\n",
    "lw_alldates_weights_grouped_merged=lw_alldates.merge(lw_alldates_weights_grouped, on=['final_locus_population_id','event_date'], how='left')\n",
    "lw_alldates_weights_grouped_merged['weight']=lw_alldates_weights_grouped_merged['weight0_x']/lw_alldates_weights_grouped_merged['weight0_y']\n",
    "lw_alldates_final=lw_alldates_weights_grouped_merged[['final_locus_population_id', 'event_date', 'historic_locus_id','weight']].sort_values(by=['final_locus_population_id','event_date','historic_locus_id'])\n",
    "lw_alldates_final.historic_locus_id=lw_alldates_final.historic_locus_id.astype('int32')\n",
    "\n",
    "def expand_dates_vectorized(df):\n",
    "    df = df.loc[df.index.repeat((df.endtime - df.starttime).dt.days)]\n",
    "    df['event_date'] = df.groupby(level=0)['starttime'].transform(lambda x: x + pd.to_timedelta(np.arange(len(x)), 'D'))\n",
    "    df = df.drop(columns=['starttime', 'endtime'])\n",
    "    df = df.rename({'count_ratio': 'weight0'}, axis=1)\n",
    "    return df\n",
    "\n",
    "lw_alldates_final = []\n",
    "for lp_id, lp_df in tqdm(locus_weights.groupby('final_locus_population_id'), 'Assigning LP weights'):\n",
    "    e_df = expand_dates_vectorized(lp_df)\n",
    "    # e_df = pd.concat([expand_dates(row) for _, row in lp_df.iterrows()], ignore_index=True)\n",
    "    agg_df = e_df.groupby(['event_date', 'final_locus_population_id']).sum().reset_index()\n",
    "    agg_df_merged=e_df.merge(agg_df, on=['final_locus_population_id','event_date', 'historic_locus_id'], how='left')\n",
    "    agg_df_merged['weight']=agg_df_merged['weight0_x']/agg_df_merged['weight0_y']\n",
    "    res_df = agg_df_merged[['final_locus_population_id', 'event_date', 'historic_locus_id','weight']].sort_values(by=['final_locus_population_id','event_date','historic_locus_id'])\n",
    "    res_df.historic_locus_id=res_df.historic_locus_id.astype('int32')\n",
    "\n",
    "    lw_alldates_final.append(res_df)\n",
    "lw_alldates_final = pd.concat(lw_alldates_final, axis=0)\n",
    "lw_alldates_final.to_csv('../data/lw_alldates_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42562af0-28f2-4eac-b541-9564e4e7a2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
