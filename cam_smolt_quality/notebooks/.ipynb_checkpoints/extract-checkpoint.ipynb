{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4688bc1a-f25d-41fb-817b-dcae85925462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9144503d-a473-441a-8e94-388e0a029e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_BUFFER = False\n",
    "QUERIES_FILEPATH = 'queries/'\n",
    "CSV_BUFFER_FILEPATH = 'data/'\n",
    "\n",
    "READ_PARAMS = dict(\n",
    "    host=os.environ[\"REPLICA_DB_HOST\"],\n",
    "    dbname=os.environ[\"REPLICA_DB_NAME\"],\n",
    "    user=os.environ[\"REPLICA_DB_USERNAME\"],\n",
    "    password=os.environ[\"REPLICA_DB_PASSWORD\"],\n",
    "    port=os.environ[\"REPLICA_DB_PORT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a219c46-a7de-44be-b4b7-11d4c3410705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(filename, conn_params=None, buffer=False, save=True):\n",
    "    print(f'Reading {filename}, from {\"csv\" if buffer else \"sql\"}')\n",
    "    query_path = os.path.join(QUERIES_FILEPATH, filename+'.sql')\n",
    "    csv_path = os.path.join(CSV_BUFFER_FILEPATH, filename+'.csv')\n",
    "\n",
    "    if not buffer:\n",
    "        with open(query_path, 'r') as file:\n",
    "            query = file.read()\n",
    "        connection_string = f\"postgresql://{conn_params['user']}:{conn_params['password']}@{conn_params['host']}:{conn_params['port']}/{conn_params['dbname']}\"\n",
    "        with create_engine(connection_string).connect() as engine:\n",
    "            df = pd.read_sql(query, con=engine) \n",
    "        if save:\n",
    "            print(f'-- Saving {filename}')\n",
    "            df.to_csv(csv_path, index=False)   \n",
    "    else:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    date_cols = [col for col in df.columns if 'date' in col or 'time' in col]\n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_datetime(df[col], utc=True)\n",
    "    return df\n",
    "\n",
    "def eSFR (row):\n",
    "    w = row['open_weight']\n",
    "    t = row['degree_days']\n",
    "    yf = (.2735797591)+(-.0720137809*t)+(.0187408253*t**2)+(-.0008145337*t**3)\n",
    "    y0 = (-.79303459)+(.43059382*t)+(-.01471246*t**2)\n",
    "    log_alpha = (-7.8284505676)+(.3748824960*t)+(-.0301640851*t**2)+(.0006516355*t**3)\n",
    "    return (yf - (yf-y0)*np.exp(-np.exp(log_alpha)*w))\n",
    "\n",
    "def generate_event_dates(row):\n",
    "    event_dates = pd.date_range(row['transfer_date'], row['sw90_date'], freq='D')\n",
    "    return pd.DataFrame({'locus_id': row['locus_id'],\n",
    "                         'fish_group_id': row['fish_group_id'],\n",
    "                         'transfer_year': row['transfer_year'],\n",
    "                         'event_date': event_dates})\n",
    "\n",
    "def weighted_avg(x, weight, factor):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        tmp = x[[weight, factor]].dropna()\n",
    "        weighted_sum = (tmp[weight] * tmp[factor]).sum()\n",
    "        count_sum = tmp[weight].sum()\n",
    "        return weighted_sum / count_sum\n",
    "\n",
    "def expand_dates_vectorized(df):\n",
    "    df = df.loc[df.index.repeat((df.endtime - df.starttime).dt.days)]\n",
    "    df['event_date'] = df.groupby(level=0)['starttime'].transform(lambda x: x + pd.to_timedelta(np.arange(len(x)), 'D'))\n",
    "    df = df.drop(columns=['starttime', 'endtime'])\n",
    "    df = df.rename({'count_ratio': 'weight0'}, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db459a7-a72d-4948-9701-b49f725ff699",
   "metadata": {},
   "source": [
    "### 1. base/20221222 Smolt performance Phase 3_copy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036e3766-ba7c-488e-8e4b-53d5a6f04445",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading mortality_target_to_be_grouped, from csv\n",
      "Reading eb_stocking_edited2, from csv\n",
      "Reading from_locus_name_lookup, from csv\n",
      "Saving smolt_dataset_transfers.csv to data folder\n"
     ]
    }
   ],
   "source": [
    "df1 = read_file('mortality_target_to_be_grouped', READ_PARAMS, buffer=True) # data on first 90 days after transfer #e)\n",
    "df2 = read_file('eb_stocking_edited2', READ_PARAMS, buffer=True)\n",
    "freshwater_names=read_file('from_locus_name_lookup', READ_PARAMS, buffer=True)\n",
    "\n",
    "df1.drop(columns=['event_date.1', 'locus_id.1', 'fish_group_id.1'],inplace=True)\n",
    "df2.from_date=pd.to_datetime(df2.from_date,format='%Y-%m-%d')\n",
    "df2.to_date=pd.to_datetime(df2.to_date,format='%Y-%m-%d')\n",
    "df2.transfer_date=pd.to_datetime(df2.transfer_date,format='%Y-%m-%d')\n",
    "df2['days_btw_to_from']=(df2.to_date-df2.from_date).dt.days\n",
    "df2['days_btw_to_transfer']=(df2.to_date-df2.transfer_date).dt.days\n",
    "weight_bins = np.linspace(100, 225, num=6)\n",
    "df2['to_avg_weight_binned'] = pd.cut(df2['to_avg_weight'], weight_bins)\n",
    "\n",
    "df1['total_mortality_perc_90']=df1['total_mortality']/df1['total_count']\n",
    "df1['transport_mortality_perc_90']=df1['transport_mortality']/df1['total_count']\n",
    "df1['nontransport_mortality_perc_90']=df1['nontransport_mortality']/df1['total_count']\n",
    "\n",
    "df1.transfer_date=pd.to_datetime(df1.transfer_date,format='%Y-%m-%d')\n",
    "df1['transfer_year']=df1.transfer_date.dt.year\n",
    "df1['transfer_month']=df1.transfer_date.dt.month\n",
    "df1['transfer_month_year']=df1['transfer_month'].astype(str)+'_'+df1['transfer_year'].astype(str)\n",
    "season_dic = {1: 'winter',2: 'spring',3: 'summer',4: 'autumn'}\n",
    "df1['transfer_season']=(df1['transfer_date'].dt.month%12 // 3 + 1).apply(lambda x: season_dic[x])\n",
    "season_dic2 = {1: 'Dec-Feb',2: 'Mar-May',3: 'Jun-Aug',4: 'Sep-Nov'}\n",
    "df1['transfer_season2']=(df1['transfer_date'].dt.month%12 // 3 + 1).apply(lambda x: season_dic2[x])\n",
    "reverse_season_dic = {v: k for k, v in season_dic.items()}\n",
    "\n",
    "df1.event_date=pd.to_datetime(df1.event_date,format='%Y-%m-%d')\n",
    "mortality=df1.dropna().groupby(['locus_id','fish_group_id']).agg({'transfer_year':'min'\n",
    "                                                                 ,'transfer_month':'min'\n",
    "                                                                 ,'transfer_month_year':'min' \n",
    "                                                                 ,'transfer_season':'min'\n",
    "                                                                 ,'transfer_season2':'min'\n",
    "                                                                 ,'total_count':'mean'\n",
    "                                                                 ,'total_mortality_perc_90':'sum'\n",
    "                                                                 ,'transport_mortality_perc_90':'sum'\n",
    "                                                                 ,'nontransport_mortality_perc_90':'sum'\n",
    "                                                                 }).reset_index()\n",
    "mortality = mortality[mortality.total_count <= mortality.total_count.quantile(.975)]\n",
    "mortality = mortality[mortality.total_count > 10000]\n",
    "mortality = mortality[mortality.transport_mortality_perc_90 < mortality.transport_mortality_perc_90.quantile(.995)]\n",
    "mortality = mortality[mortality.total_mortality_perc_90 < mortality.total_mortality_perc_90.quantile(.99)]\n",
    "mortality = mortality[mortality.nontransport_mortality_perc_90 < mortality.nontransport_mortality_perc_90.quantile(.99)]\n",
    "\n",
    "df3 = mortality.merge(df2, how='left', left_on=['locus_id', 'fish_group_id'], right_on=['to_locus_id', 'to_fish_group_id'])\n",
    "# df3 = mortality_grand.merge(df2, how='left', left_on=['locus_id', 'fish_group_id'], right_on=['to_locus_id', 'to_fish_group_id'])\n",
    "print('Saving smolt_dataset_transfers.csv to data folder')\n",
    "df3.drop(columns=['to_avg_weight_binned']).to_csv('data\\\\smolt_dataset_transfers.csv',index=False) #_until2023Feb28_short"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d537e-7a85-4c07-a0c7-5c3e56f250ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.base/20230712 Target comparison_only_mortality_nSFR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753954e2-70a8-4204-a393-b06d4df88317",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading evt_inventory_only_SW_cages_only_since_2017, from csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmitrii\\AppData\\Local\\Temp\\ipykernel_7204\\1020076031.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={'to_avg_weight':'stocking_weight'},inplace=True)\n",
      "C:\\Users\\dmitrii\\AppData\\Local\\Temp\\ipykernel_7204\\3487603803.py:28: RuntimeWarning: overflow encountered in exp\n",
      "  return (yf - (yf-y0)*np.exp(-np.exp(log_alpha)*w))\n",
      "C:\\Users\\dmitrii\\AppData\\Local\\Temp\\ipykernel_7204\\1020076031.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tmpp['transfer_date']=pd.to_datetime(tmpp['transfer_date'], utc=True)\n",
      "C:\\Users\\dmitrii\\AppData\\Local\\Temp\\ipykernel_7204\\1020076031.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tmpp['sw90_date'] = tmpp['transfer_date'] + pd.Timedelta(90,'d')\n"
     ]
    }
   ],
   "source": [
    "# tgc = pd.read_csv('data/transfers_until2023Feb28_with_sw_growth_targets.csv')\n",
    "mortality=pd.read_csv('../data/smolt_dataset_transfers.csv') #new/ _until2023May18_short\n",
    "inv=read_file('evt_inventory_only_SW_cages_only_since_2017', READ_PARAMS, buffer=True)\n",
    "\n",
    "key_columns = ['locus_id','fish_group_id','transfer_year',] \n",
    "df=mortality[key_columns+['to_avg_weight','total_mortality_perc_90','transport_mortality_perc_90','nontransport_mortality_perc_90']]\n",
    "df.rename(columns={'to_avg_weight':'stocking_weight'},inplace=True)\n",
    "\n",
    "inv['open_biomass_kg']=inv['open_count']*inv['open_weight']/1000\n",
    "inv['oSFR'] = np.where(inv['open_biomass_kg'] == 0, np.nan, inv['feed_amount'] / inv['open_biomass_kg'] * 100)\n",
    "inv['eSFR'] = inv.apply(eSFR,axis=1)\n",
    "inv['nSFR'] = np.where(inv['eSFR'] == 0, np.nan, inv['oSFR'] / inv['eSFR'])\n",
    "\n",
    "#creating new dataframe with 90 dates for each transfer\n",
    "tmpp=mortality[key_columns+['transfer_date']]\n",
    "tmpp['transfer_date']=pd.to_datetime(tmpp['transfer_date'], utc=True)\n",
    "tmpp['sw90_date'] = tmpp['transfer_date'] + pd.Timedelta(90,'d')\n",
    "\n",
    "# Apply the function to each row and concatenate the results\n",
    "new_df = pd.concat(tmpp.apply(generate_event_dates, axis=1).tolist(), ignore_index=True)\n",
    "\n",
    "inv_grouped=inv.groupby(['event_date','locus_id'])[['oSFR','eSFR','nSFR']].max().reset_index()\n",
    "df_daily = new_df.merge(mortality[key_columns+['transfer_date']]).merge(inv_grouped, how='left')\n",
    "df_daily['transfer_date']=pd.to_datetime(df_daily['transfer_date'], utc=True)\n",
    "df_daily=df_daily[df_daily.transfer_date < df_daily.event_date]\n",
    "df_daily['nSFR'] = np.where(df_daily['eSFR'] < 0, np.nan, df_daily['nSFR'])\n",
    "df_daily['oSFR'] = df_daily['oSFR'].fillna(0)\n",
    "\n",
    "df=df.merge(df_daily.groupby(key_columns)[['oSFR','nSFR']].mean().reset_index())\n",
    "df['log_mortality']=np.log(df['total_mortality_perc_90'])\n",
    "df.to_csv('data/targets.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50df74-cb81-4a45-95d8-b0861ba09756",
   "metadata": {},
   "source": [
    "## 3. base (dmitrii)/FW data processing (Dmitriis optimization).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40791ae5-cfc2-4d6f-8ad7-e3bb038e0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECALCULATE_WEIGHTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3af4f5-67ea-4774-9474-8c622ba9b154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading evt_movement_ratio_with_dates, from csv\n",
      "Reading temperature_for_CAM, from csv\n",
      "Reading locus_locus_group_matching, from csv\n",
      "Reading FW_cycle_dates, from csv\n",
      "Reading seawater_freshwater_matching, from csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "locus_weights = read_file('evt_movement_ratio_with_dates', READ_PARAMS, buffer=True)\n",
    "temperature = read_file('temperature_for_CAM', READ_PARAMS, buffer=True)\n",
    "temperature.locus_group_id=temperature.locus_group_id.astype('int16')\n",
    "#not sure in row below as it 'converts' 12.3 -> 12.296875\n",
    "temperature.value=temperature.value.astype('float16')\n",
    "temperature['event_year']=temperature['event_date'].dt.year\n",
    "\n",
    "llg_match = read_file('locus_locus_group_matching', READ_PARAMS, buffer=True)\n",
    "llg_match.locus_id=llg_match.locus_id.astype('int32')\n",
    "llg_match.locus_group_id=llg_match.locus_group_id.astype('int16')\n",
    "\n",
    "df_dates = read_file('FW_cycle_dates', READ_PARAMS, buffer=True)\n",
    "    \n",
    "sfm = read_file('seawater_freshwater_matching', READ_PARAMS, buffer=True)\n",
    "sfm_ = sfm[sfm.origin_site_type=='Freshwater'][['target_seawater_locus_id','transport_date','ponding_date','pretransfer_fw_locus_population_id','fish_count_shipped_out','avg_weight_g_stocked']]\n",
    "sfm_.pretransfer_fw_locus_population_id=sfm_.pretransfer_fw_locus_population_id.astype('int64')\n",
    "\n",
    "lw_dates=locus_weights.groupby('final_locus_population_id').agg({'starttime':'min','endtime':'max'})\n",
    "lw_dates.starttime = pd.to_datetime(lw_dates.starttime,format='%Y-%m-%d')\n",
    "lw_dates.endtime = pd.to_datetime(lw_dates.endtime,format='%Y-%m-%d')\n",
    "\n",
    "#to be checked\n",
    "lw_dates['FW_cycle_length'] = (lw_dates.endtime - lw_dates.starttime).dt.days+1\n",
    "lw_dates['starttime_year']=lw_dates['starttime'].dt.year\n",
    "#we limit FW cycles to those started in 2017 because there are issues with temperature readings for 2015-2016\n",
    "lw_dates_2017=lw_dates[lw_dates.starttime_year>=2017]\n",
    "\n",
    "N = len(locus_weights['final_locus_population_id'].unique())\n",
    "rnd_idxs = np.random.randint(0, N, 50)\n",
    "mask = locus_weights['final_locus_population_id'].unique()[rnd_idxs]\n",
    "mask = locus_weights['final_locus_population_id'].isin(mask)\n",
    "\n",
    "if RECALCULATE_WEIGHTS:\n",
    "\n",
    "    print('Processing weights in locuses...')\n",
    "    #time consuming\n",
    "    lw_alldates_list = []\n",
    "    for ind, row in tqdm(locus_weights[mask].iterrows()):\n",
    "        for d in pd.date_range(row.starttime,row.endtime-datetime.timedelta(days=1)):\n",
    "            lw_alldates_list.append([row.final_locus_population_id,d,row.historic_locus_id,row.count_ratio])\n",
    "    lw_alldates = pd.DataFrame(lw_alldates_list, columns = ['final_locus_population_id','event_date','historic_locus_id','weight0'])\n",
    "    lw_alldates_weights_grouped=lw_alldates.groupby(['final_locus_population_id','event_date'])[['weight0']].sum().reset_index()\n",
    "    lw_alldates_weights_grouped_merged=lw_alldates.merge(lw_alldates_weights_grouped, on=['final_locus_population_id','event_date'], how='left')\n",
    "    lw_alldates_weights_grouped_merged['weight']=lw_alldates_weights_grouped_merged['weight0_x']/lw_alldates_weights_grouped_merged['weight0_y']\n",
    "    lw_alldates_final=lw_alldates_weights_grouped_merged[['final_locus_population_id', 'event_date', 'historic_locus_id','weight']].sort_values(by=['final_locus_population_id','event_date','historic_locus_id'])\n",
    "    lw_alldates_final.historic_locus_id=lw_alldates_final.historic_locus_id.astype('int32')\n",
    "\n",
    "    lw_alldates_final = []\n",
    "    for lp_id, lp_df in tqdm(locus_weights.groupby('final_locus_population_id'), 'Assigning LP weights'):\n",
    "        e_df = expand_dates_vectorized(lp_df)\n",
    "        # e_df = pd.concat([expand_dates(row) for _, row in lp_df.iterrows()], ignore_index=True)\n",
    "        agg_df = e_df.groupby(['event_date', 'final_locus_population_id']).sum().reset_index()\n",
    "        agg_df_merged=e_df.merge(agg_df, on=['final_locus_population_id','event_date', 'historic_locus_id'], how='left')\n",
    "        agg_df_merged['weight']=agg_df_merged['weight0_x']/agg_df_merged['weight0_y']\n",
    "        res_df = agg_df_merged[['final_locus_population_id', 'event_date', 'historic_locus_id','weight']].sort_values(by=['final_locus_population_id','event_date','historic_locus_id'])\n",
    "        res_df.historic_locus_id=res_df.historic_locus_id.astype('int32')\n",
    "\n",
    "        lw_alldates_final.append(res_df)\n",
    "    lw_alldates_final = pd.concat(lw_alldates_final, axis=0)\n",
    "    lw_alldates_final.to_csv('data/lw_alldates_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77789ab2-0972-4b80-86f3-148b8959bc3e",
   "metadata": {},
   "source": [
    "## 4. temperature/20230607 FW data processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c8f93-4174-41b7-8797-f5eb9e735133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lw_alldates_final=read_file('lw_alldates_final',buffer=True)\n",
    "lw_dates=locus_weights.groupby('final_locus_population_id').agg({'starttime':'min','endtime':'max'})\n",
    "lw_dates.starttime = pd.to_datetime(lw_dates.starttime,format='%Y-%m-%d')\n",
    "lw_dates.endtime = pd.to_datetime(lw_dates.endtime,format='%Y-%m-%d')\n",
    "#to be checked\n",
    "lw_dates['FW_cycle_length'] = (lw_dates.endtime - lw_dates.starttime).dt.days+1\n",
    "lw_dates['starttime_year']=lw_dates['starttime'].dt.year\n",
    "#we limit FW cycles to those started in 2017 because there are issues with temperature readings for 2015-2016\n",
    "lw_dates_2017=lw_dates[lw_dates.starttime_year>=2017]\n",
    "lw_alldates_final['event_date'] = pd.to_datetime(lw_alldates_final['event_date'], utc=True)\n",
    "\n",
    "lw_alldates_final_=lw_alldates_final.merge(llg_match,left_on='historic_locus_id', right_on='locus_id', how='left')\n",
    "#alternatively rename column before merging. Thus not having to drop column thereafter\n",
    "lw_alldates_final_.drop(columns='locus_id',inplace=True)\n",
    "lw_alldates_final=[]\n",
    "\n",
    "lw_alldates_final__=lw_alldates_final_.merge(temperature[['locus_group_id', 'event_date', 'value']],how='left')\n",
    "lw_alldates_final__['event_year']=lw_alldates_final__['event_date'].dt.year\n",
    "lw_alldates_final__=lw_alldates_final__[(lw_alldates_final__['value'].notna())]\n",
    "\n",
    "lw_alldates_final__.rename(columns={'value':'temperature'},inplace=True)\n",
    "lw_alldates_final__.temperature=lw_alldates_final__.temperature.astype('float32').round(1)\n",
    "lw_alldates_final__['weight_temperature']=lw_alldates_final__['weight']*lw_alldates_final__['temperature']\n",
    "\n",
    "dft=lw_alldates_final__.groupby(['final_locus_population_id','event_date'])[['weight_temperature']].agg(lambda x: x.sum(skipna=False)).reset_index()\n",
    "dft.rename(columns={'weight_temperature':'temperature'},inplace=True)\n",
    "dft['temperature']=dft['temperature'].round(1).astype('str')\n",
    "\n",
    "\n",
    "df_dates_2017=df_dates.merge(lw_dates_2017.reset_index()[['final_locus_population_id']],left_on='pretransfer_fw_locus_population_id',right_on='final_locus_population_id',how='inner')\n",
    "df_dates_2017.drop(columns=['final_locus_population_id'],inplace=True)\n",
    "\n",
    "tmp_list=[]\n",
    "for ind,row in df_dates_2017.iterrows():\n",
    "    lp = row.pretransfer_fw_locus_population_id\n",
    "    start = row.first_movement_date\n",
    "    end = row.shipout_date\n",
    "    for d in pd.date_range(start,end):\n",
    "        tmp_list.append([lp,d])\n",
    "tmp_df=pd.DataFrame(tmp_list,columns=['final_locus_population_id','event_date'])\n",
    "dft_=tmp_df.merge(dft,how='left')\n",
    "#interpolation method #1 without handling outliers\n",
    "output_df_temp = pd.DataFrame()\n",
    "for ind,curr_df in dft_.groupby('final_locus_population_id'):\n",
    "    tmp_df=curr_df.copy()\n",
    "    tmp_df.temperature=curr_df.temperature.interpolate()\n",
    "    output_df_temp=pd.concat([output_df_temp,tmp_df])\n",
    "dft_filled = output_df_temp.copy()\n",
    "dft_filled.to_csv('data\\\\FW_temperature_filled.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94ae1d-f4a3-474e-88f8-9e1cdfda0131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def size_in_human_readable(size_in_bytes):\n",
    "    for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size_in_bytes < 1024:\n",
    "            return f\"{size_in_bytes:.2f} {unit}\"\n",
    "        size_in_bytes /= 1024\n",
    "\n",
    "# Create a list of names and objects, excluding system-defined objects\n",
    "global_objects = [(name, obj) for name, obj in globals().items() if not name.startswith('_')]\n",
    "\n",
    "# Sort the list by the size of the objects in descending order\n",
    "sorted_objects = sorted(global_objects, key=lambda x: sys.getsizeof(x[1]), reverse=True)\n",
    "\n",
    "# Print the sorted objects with their sizes\n",
    "for name, obj in sorted_objects:\n",
    "    print(f\"Object: {name}, Size: {size_in_human_readable(sys.getsizeof(obj))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9b44196-f172-449d-a982-adc55ba51acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292fdd9-f2de-486a-b265-81e1ffeaf82d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
